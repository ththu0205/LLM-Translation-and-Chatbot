{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://172.16.23.4:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "\n",
    "# ====== LOAD MODEL & DATA LÊN RAM KHI SERVER START ======\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Giả sử bạn đã có file \"indexed_data.pkl\" chứa:\n",
    "#   [\n",
    "#       {\n",
    "#           \"header_index\": ...,\n",
    "#           \"header\": ...,\n",
    "#           \"text\": ...,\n",
    "#           \"embedding\": np.array(...)\n",
    "#       },\n",
    "#       ...\n",
    "#   ]\n",
    "with open(\"indexed_data.pkl\", \"rb\") as f:\n",
    "    indexed_data = pickle.load(f)\n",
    "\n",
    "# Tạo list embeddings\n",
    "content_embeddings_array = np.vstack([item[\"embedding\"] for item in indexed_data])\n",
    "headers = [item[\"header\"] for item in indexed_data]\n",
    "texts = [item[\"text\"] for item in indexed_data]\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "def find_best_chunks(question, content_weight=1.0, k=2):\n",
    "    \"\"\"\n",
    "    Lấy top-k đoạn văn phù hợp nhất từ 'indexed_data' dựa trên câu hỏi.\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed câu hỏi\n",
    "    question_embedding = model.encode([question])\n",
    "\n",
    "    # Tính độ tương đồng với mỗi chunk\n",
    "    content_similarities = cosine_similarity(question_embedding, content_embeddings_array)[0]\n",
    "\n",
    "    # Lấy top-k\n",
    "    sorted_indices = np.argsort(content_similarities)[::-1]  # descending\n",
    "    top_indices = sorted_indices[:k]\n",
    "\n",
    "    top_chunks = [indexed_data[i] for i in top_indices]\n",
    "    return top_chunks\n",
    "\n",
    "def generate_answer(question, chunks):\n",
    "    \"\"\"\n",
    "    Tạm minh họa: ghép context + question rồi trả về 1 string làm câu trả lời.\n",
    "    Có thể thay bằng LLM (Gemini/OpenAI/…).\n",
    "    \"\"\"\n",
    "    # Ghép tất cả text trong chunks làm context\n",
    "    context = \"\\n\\n---\\n\\n\".join([chunk[\"text\"] for chunk in chunks])\n",
    "    \n",
    "    # Ví dụ logic đơn giản: Trả về (context + \" => answer\"). \n",
    "    # Thực tế bạn dùng LLM (Gemini, OpenAI) thay thế tùy ý.\n",
    "    final_answer = f\"[Context]\\n{context}\\n\\n[User Question]\\n{question}\\n\\n[Answer]\\nThis is a mock answer from local Python server.\"\n",
    "    return final_answer\n",
    "\n",
    "\n",
    "# ====== TẠO ENDPOINT CHÍNH CHO FRONTEND GỌI ======\n",
    "@app.route(\"/ask\", methods=[\"POST\"])\n",
    "def ask_chatbot():\n",
    "    data = request.json\n",
    "    question = data.get(\"question\", \"\")\n",
    "\n",
    "    # Logic truy xuất\n",
    "    top_chunks = find_best_chunks(question, k=2)\n",
    "    answer_text = generate_answer(question, top_chunks)\n",
    "\n",
    "    return jsonify({\"answer\": answer_text})\n",
    "\n",
    "# ====== CHẠY SERVER ======\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=5000, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
